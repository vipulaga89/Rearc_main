{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruMd6uL58m1"
      },
      "source": [
        "**Rearc assignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 0 - pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from io import StringIO\n",
        "from bs4 import BeautifulSoup\n",
        "from botocore.exceptions import NoCredentialsError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1 - Using the data to print sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sb8O2QjlrAJp",
        "outputId": "4a6f1ad6-1bb7-4ff5-c248-cd0d61f41f09"
      },
      "outputs": [],
      "source": [
        "# === Configuration ===\n",
        "bucket_name = \"rearc-assignment-vipul\"\n",
        "s3 = boto3.client(\"s3\")\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/117.0 Safari/537.36\"}\n",
        "bls_base_url = \"https://download.bls.gov/pub/time.series/pr/\"\n",
        "population_api_url = \"https://datausa.io/api/data?drilldowns=Nation&measures=Population\"\n",
        "population_key = \"datausa/population_1.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Part 1: Sync BLS Files to S3 ===\n",
        "def get_remote_bls_files():\n",
        "    print(\"Fetching BLS index page...\")\n",
        "    response = requests.get(bls_base_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    return [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\").startswith(\"pr.\")]\n",
        "\n",
        "def get_existing_s3_files():\n",
        "    s3_keys = []\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(Bucket=bucket_name, Prefix=\"bls/\"):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            s3_keys.append(obj[\"Key\"].replace(\"bls/\", \"\"))\n",
        "    return s3_keys\n",
        "\n",
        "def sync_bls_to_s3():\n",
        "    remote_files = get_remote_bls_files()\n",
        "    existing_files = get_existing_s3_files()\n",
        "\n",
        "    for filename in remote_files:\n",
        "        if filename not in existing_files:\n",
        "            print(f\"Uploading: {filename}\")\n",
        "            try:\n",
        "                file_response = requests.get(bls_base_url + filename, headers=headers)\n",
        "                s3.put_object(Bucket=bucket_name, Key=f\"bls/{filename}\", Body=file_response.content)\n",
        "                print(f\"Uploaded: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Skipping already-uploaded file: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMvyt-xMwpuz"
      },
      "source": [
        "Part 2 - Using the data to push it over the S3 bucket in a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIf-0ZMBv7zg",
        "outputId": "ecd6e72d-a468-4c47-e90a-94bebec541b0"
      },
      "outputs": [],
      "source": [
        "def fetch_population_data():\n",
        "    print(\"Fetching population data from API...\")\n",
        "    response = requests.get(population_api_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(\"Failed to fetch population data\")\n",
        "    s3.put_object(\n",
        "        Bucket=bucket_name,\n",
        "        Key=population_key,\n",
        "        Body=json.dumps(response.json()),\n",
        "        ContentType='application/json'\n",
        "    )\n",
        "    print(f\"Population data uploaded to S3: {population_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-CbqdM1F7Z1"
      },
      "source": [
        "Part 3 - Copying the pr.data.0.Current data: Tried using the data file from the server, but google result showed, the needs to be pulled from the server for working with google colab. So changed the interpreter to Visual studio to see the API call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "-mL0RE3gDHzb",
        "outputId": "b20f90dd-2580-496d-ee0a-8c20e5f23da3"
      },
      "outputs": [],
      "source": [
        "def perform_analysis():\n",
        "    print(\"Starting Part 3: Data Analysis...\")\n",
        "\n",
        "    # Load the population JSON file from S3\n",
        "    try:\n",
        "        pop_response = s3.get_object(Bucket=bucket_name, Key=population_key)\n",
        "        pop_json = json.loads(pop_response['Body'].read())\n",
        "        pop_df = pd.DataFrame(pop_json['data'])\n",
        "    except Exception as e:\n",
        "        print(\"Failed to read population data from S3:\", e)\n",
        "        return\n",
        "\n",
        "    # Filter for years 2013 to 2018 and compute stats\n",
        "    pop_df[\"Year\"] = pop_df[\"Year\"].astype(int)\n",
        "    pop_range = pop_df[(pop_df[\"Year\"] >= 2013) & (pop_df[\"Year\"] <= 2018)]\n",
        "    mean_val = pop_range[\"Population\"].astype(int).mean()\n",
        "    std_val = pop_range[\"Population\"].astype(int).std()\n",
        "\n",
        "    print(f\"Mean population (2013-2018): {int(mean_val)}\")\n",
        "    print(f\"Standard deviation: {int(std_val)}\")\n",
        "\n",
        "    # Load the BLS data from S3\n",
        "    bls_key = \"datausa/pr.data.0.Current\"\n",
        "    try:\n",
        "        bls_obj = s3.get_object(Bucket=bucket_name, Key=bls_key)\n",
        "        raw_txt = bls_obj['Body'].read().decode(\"utf-8\")\n",
        "        bls_df = pd.read_csv(StringIO(raw_txt), sep=\"\\t\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to read BLS data from S3:\", e)\n",
        "        return\n",
        "\n",
        "    # Clean and process BLS data\n",
        "    bls_df = bls_df.dropna()\n",
        "    bls_df.columns = [col.strip().lower() for col in bls_df.columns]\n",
        "\n",
        "    bls_df['value'] = pd.to_numeric(bls_df['value'], errors='coerce')\n",
        "    bls_df = bls_df.dropna(subset=['value'])\n",
        "\n",
        "    grouped = bls_df.groupby(['series_id', 'year'])['value'].sum().reset_index()\n",
        "    best_years = grouped.loc[grouped.groupby('series_id')['value'].idxmax()]\n",
        "\n",
        "    print(\"\\nTop year by series ID:\")\n",
        "    print(best_years.head())\n",
        "\n",
        "    # Join with population data for a specific filter\n",
        "    match_df = bls_df[(bls_df['series_id'] == 'PRS30006032') & (bls_df['period'].str.strip() == 'Q01')]\n",
        "    match_df = match_df[['series_id', 'year', 'period', 'value']]\n",
        "\n",
        "    pop_df.rename(columns={'Year': 'year'}, inplace=True)\n",
        "    pop_df['year'] = pop_df['year'].astype(int)\n",
        "\n",
        "    final_report = pd.merge(match_df, pop_df[['year', 'Population']], on='year', how='left')\n",
        "\n",
        "    print(\"\\nReport for series_id='PRS30006032' and period='Q01':\")\n",
        "    print(final_report)\n",
        "\n",
        "    # Optionally write back to S3 or local file if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching BLS index page...\n",
            "Fetching population data from API...\n",
            "Population data uploaded to S3: datausa/population_1.json\n",
            "Performing analytics...\n",
            "Mean Population (2013-2018): 317437383.0\n",
            "Std Population (2013-2018): 4257089.5415293295\n",
            "\n",
            "Series PRS30006032 - Q01 vs Population:\n",
            "Empty DataFrame\n",
            "Columns: [series_id, year, period, value, Population]\n",
            "Index: []\n",
            "\n",
            "Best Year per Series ID:\n",
            "             series_id  year    value\n",
            "27   PRS30006011        2022   20.500\n",
            "57   PRS30006012        2022   17.100\n",
            "63   PRS30006013        1998  705.895\n",
            "105  PRS30006021        2010   17.700\n",
            "135  PRS30006022        2010   12.400\n",
            "\n",
            "Best Year per Series ID:\n",
            "             series_id  year    value\n",
            "27   PRS30006011        2022   20.500\n",
            "57   PRS30006012        2022   17.100\n",
            "63   PRS30006013        1998  705.895\n",
            "105  PRS30006021        2010   17.700\n",
            "135  PRS30006022        2010   12.400\n",
            "\n",
            " All parts completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# === Entrypoint ===\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        sync_bls_to_s3()\n",
        "        fetch_population_data()\n",
        "        perform_analysis()\n",
        "        print(\"\\n All parts completed successfully.\")\n",
        "    except NoCredentialsError:\n",
        "        print(\"AWS credentials not found. Make sure your environment is configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
